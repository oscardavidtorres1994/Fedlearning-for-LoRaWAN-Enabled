{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c80d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "from multiprocessing import Process\n",
    "import gc\n",
    "\n",
    "import flwr as fl\n",
    "from flwr.server.strategy import FedAvg\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "#!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/\n",
    "# demonstration of calculating metrics for a neural network model using sklearn\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from flwr.server.client_manager import SimpleClientManager\n",
    "from flwr.server.criterion import Criterion\n",
    "from flwr.common.logger import log\n",
    "from flwr.server.client_manager import ClientManager\n",
    "from flwr.server.client_proxy import ClientProxy\n",
    "from flwr.common import (\n",
    "    EvaluateIns,\n",
    "    EvaluateRes,\n",
    "    FitIns,\n",
    "    FitRes,\n",
    "    MetricsAggregationFn,\n",
    "    NDArrays,\n",
    "    Parameters,\n",
    "    Scalar,\n",
    "    ndarrays_to_parameters,\n",
    "    parameters_to_ndarrays,\n",
    ")\n",
    "\n",
    "# Make TensorFlow log less verbose\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2257ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# argumentos\n",
    "n = len(sys.argv)\n",
    "print(\"Total arguments passed:\", n)\n",
    "iteracoes = 0\n",
    "finalIterations = 0\n",
    "if(n > 0):\n",
    "    for value in sys.argv:\n",
    "        print(\"arg:\", value)\n",
    "        try:\n",
    "            iteracoes = int(value)\n",
    "            break\n",
    "        except:\n",
    "            print(\"no\")\n",
    "print(\"iteracoes:\",0)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4484bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# General configuration\n",
    "NUM_EPOCHS =80\n",
    "NUMBER_OF_ITERATIONS_FINAL = 1\n",
    "execution=12\n",
    "    \n",
    "\n",
    "BATCH_SIZE = 16\n",
    "VERBOSE = 0\n",
    "\n",
    "\n",
    "clientId_TO_be_removed = -1\n",
    "\n",
    "\n",
    "\n",
    "outputFolder = \"FL_\"+str(execution)+\"_ep_\"+str(NUM_EPOCHS)+\"_rd_\"+str(NUMBER_OF_ITERATIONS_FINAL)\n",
    "\n",
    "\n",
    "# train file name modifier\n",
    "fileSufixTrain = \"\" # _smote for smote\n",
    "\n",
    "fl.common.logger.configure(identifier=\"myFlowerExperiment\", filename=\"log_\"+outputFolder+\".txt\")\n",
    "\n",
    "# usado para checkpoints\n",
    "if(iteracoes > 0):\n",
    "    NUMBER_OF_ITERATIONS_FINAL = iteracoes\n",
    "    \n",
    "NUMBER_OF_ITERATIONS = NUMBER_OF_ITERATIONS_FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0929b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Checking whether the folder exists or not\")\n",
    "isExist = os.path.exists(outputFolder)\n",
    "if not isExist:\n",
    "    # Create a new directory because it does not exist\n",
    "    os.makedirs(outputFolder)\n",
    "    print(\"The new directory is created!\")\n",
    "else:\n",
    "    print(\"The directory exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75b3967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected features\n",
    "inputFeatures = ['battery_potential', \"consumption\", \"ignition\", \"motor_water_temp\", \"oil_pressure\"]\n",
    "outputClasses = [\"normal_label\"]\n",
    "#outputClasses = [\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa91419-07e8-4cfb-b341-11a8d7d2334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_complete=pd.read_csv('Complete.csv', index_col=0)\n",
    "X_test=pd.read_csv('Test.csv', index_col=0)\n",
    "X_val=pd.read_csv('Validation.csv', index_col=0)\n",
    "X_train=pd.read_csv('Train.csv', index_col=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_merge=pd.concat([X_train, X_val], axis=0)\n",
    "print(len(X_merge))\n",
    "\n",
    "index_to_train = X_merge.index\n",
    "\n",
    "x_complete_filtered=df_complete.loc[index_to_train]\n",
    "\n",
    "for column in inputFeatures:\n",
    "    scaler.fit(x_complete_filtered[column].values.reshape(-1, 1))\n",
    "    x_complete_filtered[column]=scaler.fit_transform(x_complete_filtered[column].values.reshape(-1, 1))\n",
    "    X_test[column]=scaler.fit_transform(X_test[column].values.reshape(-1, 1))\n",
    "\n",
    "clientList=[]\n",
    "clientList = [group for _, group in x_complete_filtered.groupby('device_id')]\n",
    "\n",
    "for idx, df in enumerate(clientList, start=1):\n",
    "    print(f\"DataFrame {idx} (device_id={df['device_id'].iloc[0]}):\\n{df}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843185ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for df in clientList:\n",
    "    print(Counter(df['device_id']))\n",
    "    next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb8ae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[inputFeatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654f6e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clientList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a8bdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateMetrics(y_test,yhat_probs):\n",
    "    # predict crisp classes for test set deprecated\n",
    "    #yhat_classes = model.predict_classes(X_test, verbose=0)\n",
    "    #yhat_classes = np.argmax(yhat_probs,axis=1)\n",
    "    yhat_classes = yhat_probs.round()\n",
    "    # accuracy: (tp + tn) / (p + n)\n",
    "    accuracy = accuracy_score(y_test, yhat_classes)\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = precision_score(y_test, yhat_classes)\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = recall_score(y_test, yhat_classes)\n",
    "    # f1: 2 tp / (2 tp + fp + fn)\n",
    "    f1 = f1_score(y_test, yhat_classes)\n",
    "    # kappa\n",
    "    kappa = cohen_kappa_score(y_test, yhat_classes)\n",
    "    # ROC AUC\n",
    "    auc = roc_auc_score(y_test, yhat_probs)\n",
    "    # confusion matrix\n",
    "    matrix = confusion_matrix(y_test, yhat_classes)\n",
    "    #print(matrix)\n",
    "    \n",
    "    array = []\n",
    "    results = dict()\n",
    "    results['accuracy'] = accuracy\n",
    "    results['precision'] = precision\n",
    "    results['recall'] = recall\n",
    "    results['f1_score'] = f1\n",
    "    results['cohen_kappa_score'] = kappa\n",
    "    results['roc_auc_score'] = auc\n",
    "    results['matrix'] = (\"[[ \" +str(matrix[0][0]) + \" \" +str(matrix[0][1]) +\"][ \" +str(matrix[1][0]) + \" \" + str(matrix[1][1]) +\"]]\") # array.append(np.array(matrix,dtype=object))\n",
    "    results['TP'] = matrix[0][0]\n",
    "    results['FP'] = matrix[0][1]\n",
    "    results['FN'] = matrix[1][0]\n",
    "    results['TN'] = matrix[1][1]\n",
    "    \n",
    "    array.append(accuracy)\n",
    "    array.append(precision)\n",
    "    array.append(recall)\n",
    "    array.append(f1)\n",
    "    array.append(kappa)\n",
    "    array.append(auc)\n",
    "    array.append(\"[[ \" +str(matrix[0][0]) + \" \" +str(matrix[0][1]) +\"][ \" +str(matrix[1][0]) + \" \" + str(matrix[1][1]) +\"]]\") # array.append(np.array(matrix,dtype=object))\n",
    "    array.append(matrix[0][0]) # TP\n",
    "    array.append(matrix[0][1]) # FP\n",
    "    array.append(matrix[1][0]) # FN\n",
    "    array.append(matrix[1][1]) # TN\n",
    "    \n",
    "    return results, array\n",
    "\n",
    "# y_test     = Array with real values\n",
    "# yhat_probs = Array with predicted values\n",
    "def printMetrics(y_test,yhat_probs):\n",
    "    # generate metrics\n",
    "    results, array= generateMetrics(y_test,yhat_probs)\n",
    "\n",
    "    # accuracy: (tp + tn) / (p + n)\n",
    "    accuracy = results['accuracy']\n",
    "    print('Accuracy: %f' % accuracy)\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = results['precision']\n",
    "    print('Precision: %f' % precision)\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = results['recall'] \n",
    "    print('Recall: %f' % recall)\n",
    "    # f1: 2 tp / (2 tp + fp + fn)\n",
    "    f1 = results['f1_score']\n",
    "    print('F1 score: %f' % f1)\n",
    "    # kappa\n",
    "    kappa = results['cohen_kappa_score']\n",
    "    print('Cohens kappa: %f' % kappa)\n",
    "    # ROC AUC\n",
    "    auc = results['roc_auc_score']\n",
    "    print('ROC AUC: %f' % auc)\n",
    "    # confusion matrix\n",
    "    print(\"Confusion Matrix\")\n",
    "    matrix = results['matrix']\n",
    "    print(matrix)\n",
    "    \n",
    "    return results, array\n",
    "\n",
    "def generateGlobalMetrics(metrics):\n",
    "    accuracy,precision,recall,f1_score,cohen_kappa_score,roc_auc_score = 0,0,0,0,0,0\n",
    "    for metric in metrics:\n",
    "        accuracy = accuracy + metric['accuracy']\n",
    "        precision = precision + metric['precision']\n",
    "        recall = recall + metric['recall']\n",
    "        f1_score = f1_score + metric['f1_score']\n",
    "        cohen_kappa_score = cohen_kappa_score + metric['cohen_kappa_score']\n",
    "        roc_auc_score = roc_auc_score + metric['roc_auc_score']\n",
    "        \n",
    "    # mean\n",
    "    size = len(metrics)\n",
    "    print(size)\n",
    "    accuracy = accuracy / size\n",
    "    precision = precision / size\n",
    "    recall = recall / size\n",
    "    f1_score = f1_score / size\n",
    "    cohen_kappa_score = cohen_kappa_score / size\n",
    "    roc_auc_score = roc_auc_score / size\n",
    "    \n",
    "    return [accuracy,precision,recall,f1_score,cohen_kappa_score,roc_auc_score]\n",
    "\n",
    "def showGlobalMetrics(metrics):\n",
    "    res = generateGlobalMetrics(metrics)\n",
    "    \n",
    "    accuracy = res[0]\n",
    "    precision = res[1]\n",
    "    recall = res[2]\n",
    "    f1_score = res[3]\n",
    "    cohen_kappa_score = res[4]\n",
    "    roc_auc_score = res[5]\n",
    "    \n",
    "    #show:\\\n",
    "    print(\"accuracy: \",accuracy)\n",
    "    print(\"precision: \",precision)\n",
    "    print(\"recall: \",recall)\n",
    "    print(\"f1_score: \",f1_score)\n",
    "    print(\"cohen_kappa_score: \",cohen_kappa_score)\n",
    "    print(\"roc_auc_score: \",roc_auc_score)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bed070",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preparing X_train data\")\n",
    "# load cliend data\n",
    "#clientList = loadDataFromFoldersOnList(trainFolders,inputFolderPath,fileSufixTrain)\n",
    "        \n",
    "NUMBER_OF_CLIENTS = len(clientList)\n",
    "print(\"Total\",(len(clientList)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7064add",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"creating model\")\n",
    "\n",
    "def create_keras_model():\n",
    "\n",
    "    input_dim = len(inputFeatures)  # Número de características (columnas)\n",
    "    encoding_dim = 128\n",
    "\n",
    "    model=tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=(input_dim,)),\n",
    "        tf.keras.layers.Dense(encoding_dim, activation='tanh'),\n",
    "        tf.keras.layers.Dense(2, activation='tanh'),\n",
    "        tf.keras.layers.Dense(encoding_dim, activation='tanh'),\n",
    "        tf.keras.layers.Dense(input_dim, activation='tanh')\n",
    "        # layers.Dense(input_dim, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    \n",
    "\n",
    "keras_model = create_keras_model()\n",
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39a58ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_type(dataframe):\n",
    "    \n",
    "    # transform inputs\n",
    "    for column in inputFeatures:\n",
    "        dataframe[column] = dataframe[column].astype('float32')\n",
    "    \n",
    "    # transform outputs\n",
    "    for column in outputClasses:\n",
    "        dataframe[column] = dataframe[column].astype('float32')\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "# transforms the data\n",
    "X_test = transform_data_type(X_test)\n",
    "\n",
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec93564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Prepering the test dataset\")\n",
    "# selects the data to train and test\n",
    "X_test_data = X_test[inputFeatures]\n",
    "y_test_label = X_test[outputClasses]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548a9007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362b0f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_save_results(keras_model,X_test_data_, y_test_label_, current_round_index, \n",
    "                              clientId, prefix_string = \"Results\", lossValue = -1):\n",
    "    # predict values\n",
    "    \n",
    "\n",
    "    yhat_probs = keras_model.predict(X_test_data_,verbose=VERBOSE)\n",
    "    new_parameters = keras_model.get_weights()\n",
    "    size_after_training = sum(np.prod(w.shape) for w in new_parameters)\n",
    "    size_after_training_kb = size_after_training / 1024\n",
    "    \n",
    "\n",
    "    mse = np.mean(np.power(X_test_data_ - yhat_probs, 2), axis=1)\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0\n",
    "    #loss = log_loss(y_test, model.predict_proba(X_test))\n",
    "    \n",
    "    for threshold_value in range(50, 101):\n",
    "        threshold = np.percentile(mse, threshold_value)\n",
    "        anomaly_labels = (mse > threshold)\n",
    "        y_pred_bool = ~anomaly_labels\n",
    "        # y_test = X_test_data_[\"normal_label\"]\n",
    "    \n",
    "        f1 = f1_score(y_test_label_, y_pred_bool)\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "            best_threshold_value=threshold_value\n",
    "\n",
    "    \n",
    "    yhat_probs_=(mse > best_threshold)\n",
    "    yhat_probs_rounded=~yhat_probs_\n",
    "\n",
    "    y_predicted_df = pd.DataFrame(data=yhat_probs_rounded,columns=['normal_label']) \n",
    "\n",
    "    \n",
    "    roundData = []\n",
    "\n",
    "    columns = ['client','round','loss', 'size', 'class','accuracy','precision','recall', \n",
    "               'f1_score','cohen_kappa_score','roc_auc_score','confusion_matrix',\n",
    "               'TP','FP','FN','TN', ]\n",
    "    \n",
    "    # Instantiate the list that will contain the results\n",
    "    listOfMetrics = list()\n",
    "    \n",
    "    # print('awake')    \n",
    "    # res,resA = printMetrics(y_test_label['awake'],y_predicted_df['awake'])\n",
    "    res,resA = generateMetrics(y_test_label_['normal_label'],y_predicted_df['normal_label'])\n",
    "    listOfMetrics.append(res)\n",
    "    \n",
    "    classData = np.concatenate(([clientId,current_round_index,lossValue,size_after_training_kb,'normal_label'], resA))\n",
    "    roundData.append(classData)\n",
    "\n",
    "    dataMetrics = pd.DataFrame(data=roundData,columns=columns) \n",
    "    # write file\n",
    "    if(clientId >= 0):\n",
    "        outputMetricFile = outputFolder+\"/\"+prefix_string+\"_MLP_client_\" + str(clientId) + \"_round_\" + str(current_round_index) + \".csv\"\n",
    "    else:\n",
    "        #outputMetricFile = outputFolder+\"/global_model_MLP_metrics.csv\"\n",
    "        outputMetricFile = outputFolder+\"/\"+ prefix_string+ \".csv\"\n",
    "        # print global model results\n",
    "        if(os.path.isfile(outputMetricFile)):\n",
    "            dataset = pd.read_csv(outputMetricFile)\n",
    "            dataMetrics = pd.concat([dataset, dataMetrics], axis=0)\n",
    "        # Perform garbage collection\n",
    "        gc.collect()\n",
    "        \n",
    "    dataMetrics.to_csv(outputMetricFile, sep=',', encoding='utf-8', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc894c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveModelStrategy(fl.server.strategy.FedAvg):\n",
    "    def aggregate_fit(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: List[Tuple[fl.server.client_proxy.ClientProxy, fl.common.FitRes]],\n",
    "        failures: List[Union[Tuple[ClientProxy, FitRes], BaseException]],\n",
    "    ) -> Tuple[Optional[Parameters], Dict[str, Scalar]]:\n",
    "\n",
    "        # Call aggregate_fit from base class (FedAvg) to aggregate parameters and metrics\n",
    "        aggregated_parameters, aggregated_metrics = super().aggregate_fit(server_round, results, failures)\n",
    "\n",
    "        return aggregated_parameters, aggregated_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71c6242",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Declarating client function\")\n",
    "\n",
    "# Define a Flower client\n",
    "class FlowerISABELASleepClient(fl.client.NumPyClient):\n",
    "\n",
    "    def __init__(self, clientId, model, X_train_data, y_train_label,round_index=0):\n",
    "        self.round_index = round_index\n",
    "        self.clientId = clientId\n",
    "        self.model = model\n",
    "        self.X_train_data = X_train_data\n",
    "        self.y_train_label = y_train_label\n",
    "        \n",
    "\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        \"\"\"Return current weights.\"\"\"\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        \"\"\"Fit model and return new weights as well as number of training examples.\"\"\"\n",
    "\n",
    "        self.model.set_weights(parameters)\n",
    "        size_before_training = sum(np.prod(w.shape) for w in parameters)\n",
    "        \n",
    "        y_pred=self.model.predict(self.X_train_data, verbose=VERBOSE)\n",
    "        loss=np.mean(tf.keras.losses.mean_squared_error(self.X_train_data, y_pred))\n",
    "        # # print model results\n",
    "        # evaluate_and_save_results(self.model,self.X_train_data, self.y_train_label, self.round_index, self.clientId,\"Local_train_beforeFit_trainDataset\",loss)\n",
    "        evaluate_and_save_results(self.model,self.X_train_data, self.y_train_label, self.round_index, self.clientId,\"Local_before\",loss)\n",
    "        \n",
    "        # \n",
    "        #if self.clientId != \"id0004\":\n",
    "        self.model.fit(self.X_train_data, self.X_train_data, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE,verbose=VERBOSE)\n",
    "\n",
    "        y_pred=self.model.predict(self.X_train_data, verbose=VERBOSE)\n",
    "        loss=np.mean(tf.keras.losses.mean_squared_error(self.X_train_data, y_pred))\n",
    "        # # print model results\n",
    "        evaluate_and_save_results(self.model,self.X_train_data, self.y_train_label, self.round_index, self.clientId,\"Local_after\",loss)\n",
    "\n",
    "        y_pred=self.model.predict(X_test_data, verbose=VERBOSE)\n",
    "\n",
    "        loss=np.mean(tf.keras.losses.mean_squared_error(X_test_data, y_pred))\n",
    "        \n",
    "        new_parameters = self.model.get_weights()\n",
    "        size_after_training = sum(np.prod(w.shape) for w in new_parameters)\n",
    "\n",
    "        evaluate_and_save_results(self.model,X_test_data, y_test_label, self.round_index, self.clientId,\"Test_after\",loss)\n",
    "        return self.model.get_weights(), len(self.X_train_data), {}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c336f022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluate_fn( model):\n",
    "    \n",
    "    def evaluate(\n",
    "        server_round: int, parameters: NDArrays, config: Dict[str, Scalar]\n",
    "    ) -> Optional[Tuple[float, Dict[str, Scalar]]]:\n",
    "        \n",
    "        current_round = server_round\n",
    "            \n",
    "        print(\"Evaluating global model round\",current_round)\n",
    "        \n",
    "        model.set_weights(parameters)\n",
    "        \n",
    "        # Evaluate local model parameters on the local test data\n",
    "        print(X_test_data)\n",
    "        print(y_test_label)\n",
    "        y_pred=model.predict(X_test_data,verbose=VERBOSE)\n",
    "\n",
    "        loss=np.mean(tf.keras.losses.mean_squared_error(X_test_data, y_pred))\n",
    "        #loss=-1\n",
    "        accuracy=np.mean(np.equal(X_test_data, y_pred))\n",
    "        print(loss)\n",
    "        print(accuracy)\n",
    "\n",
    "        evaluate_and_save_results(model,X_test_data, y_test_label, current_round, -1,\"global_model_MLP_metrics\",loss)\n",
    "        \n",
    "        # AVALIAR A PORRA DO DATASET QUE FOI REMOVIDO da agregação\n",
    "        if(clientId_TO_be_removed >= 0):\n",
    "            data   = clientList[clientId_TO_be_removed][inputFeatures]\n",
    "            labels = clientList[clientId_TO_be_removed][outputClasses]\n",
    "\n",
    "            y_pred_New = model.predict(data, verbose=VERBOSE)\n",
    "            lossNew=np.mean(tf.keras.losses.mean_squared_error(data, y_pred_New))\n",
    "            # # print model results\n",
    "            file_name = \"excluded_model_MLP_metrics_from_client_\"+str(clientId_TO_be_removed)\n",
    "            evaluate_and_save_results(model,data, labels, current_round, -1,file_name,lossNew)\n",
    "        \n",
    "        return loss, { 'accuracy': accuracy }\n",
    "    return evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e8c638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import math\n",
    "# Create an instance of the model and get the parameters\n",
    "\n",
    "# Specify client resources if you need GPU (defaults to 1 CPU and 0 GPU)\n",
    "client_resources = None\n",
    "#if DEVICE.type == \"cuda\":\n",
    "\n",
    "client_resources = {\"num_cpus\": 1}\n",
    "\n",
    "#keras_model = create_keras_model()\n",
    "keras_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "\n",
    "def client_fn(cid) -> FlowerISABELASleepClient:\n",
    "    print(\"starting client: \"+str(cid),type(cid))\n",
    "    #convert client ID to int\n",
    "    clientId = int(cid)\n",
    "    print(\"starting client: \", type(clientId))\n",
    "\n",
    "    data   = clientList[clientId][inputFeatures]\n",
    "    labels = clientList[clientId][outputClasses]\n",
    "    \n",
    "    print(\"Creating client model to client UID: \"+str(cid))\n",
    "    print(Counter(clientList[clientId]['device_id']))\n",
    "    print(\"Data X: \"+str(len(data)))\n",
    "    print(\"Data Y: \"+str(len(labels)))\n",
    "    print(\"Infor X\"+str(data.info()))\n",
    "    print(\"Infor X\"+str(labels.info()))\n",
    "    \n",
    "    file_global_model = outputFolder+\"/global_model_MLP_metrics.csv\"\n",
    "    index_round = 0 \n",
    "    \n",
    "    # get last\n",
    "    if(os.path.isfile(file_global_model)):\n",
    "        dataset = pd.read_csv(file_global_model)\n",
    "        index_round = dataset[\"round\"].max() + 1\n",
    "        del dataset\n",
    "    \n",
    "    print(\"Creating client model to client: \"+str(cid),\"round\",index_round)\n",
    "    # Load and compile a Keras model for CIFAR-10\n",
    "    model = create_keras_model()\n",
    "    #modelcompile\"adam\", \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    return FlowerISABELASleepClient(clientId,model,data,labels,index_round)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5504e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f207ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/adap/flower/blob/main/src/py/flwr/server/criterion_test.py\n",
    "print(\"Configuring criterion selection\")\n",
    "class TestCriterion(Criterion):\n",
    "        \"\"\"Criterion to select only test clients.\"\"\"\n",
    "\n",
    "        def select(self, client: ClientProxy) -> bool:\n",
    "            \n",
    "            #print(\"Print type: \",type(client))\n",
    "            #print(\"Print dir: \",type(dir(client)))\n",
    "            #print(\"Print type client: \",type(client.client_fn))\n",
    "            \n",
    "            #print(\"Print type client: \",(client.client_fn.clientId))\n",
    "            print(\"Testing Criterion to cid: \",client.cid,\" \",(type(client.cid))) # ok, acessa\n",
    "\n",
    "\n",
    "\n",
    "            if(client.cid == str(clientId_TO_be_removed)):\n",
    "                print(\"Return false\")\n",
    "                return False\n",
    "\n",
    "            print(\"Return true\")\n",
    "            return True\n",
    "\n",
    "customCriterion = TestCriterion()\n",
    "\n",
    "min_num_clients_meus = NUMBER_OF_CLIENTS - 1\n",
    "print(\"Configuring strategy\")\n",
    "strategy = SaveModelStrategy(\n",
    "    min_available_clients=min_num_clients_meus, # menos o client ignorado na seleção\n",
    "    evaluate_fn=get_evaluate_fn(keras_model)\n",
    ") # (same arguments as FedAvg here)\n",
    "\n",
    "print(\"Min num of clients: \",min_num_clients_meus)\n",
    "\n",
    "import random\n",
    "\n",
    "# https://flower.dev/docs/framework/_modules/flwr/server/client_manager.html#ClientManager.sample \n",
    "print(\"Configuring client manager selection\")\n",
    "class CustomSimpleClientManager(SimpleClientManager):\n",
    "\n",
    "    # configurate the client selection\n",
    "    def sample(self,num_clients: int, min_num_clients: Optional[int] = None, criterion: Optional[Criterion] = None) -> List[ClientProxy]:\n",
    "        print(\"Instantiate custom criterion selection\")\n",
    "        print(\"Params: \",num_clients,\" \", min_num_clients)\n",
    "        \n",
    "        customCriterion = TestCriterion()\n",
    "\n",
    "        \"\"\"Sample a number of Flower ClientProxy instances.\"\"\"\n",
    "        # Block until at least num_clients are connected.\n",
    "        if min_num_clients is None:\n",
    "            min_num_clients = num_clients\n",
    "            \n",
    "        print(\"Params: \",num_clients,\" \", min_num_clients)\n",
    "            \n",
    "        selectedCriterion = customCriterion\n",
    "        \n",
    "        print(\"criterion parameter: \",(criterion is not None))\n",
    "        \n",
    "        # if there are no criteria, uses my custom one\n",
    "        if criterion is not None:\n",
    "            selectedCriterion = criterion\n",
    "            \n",
    "            \n",
    "        self.wait_for(min_num_clients)\n",
    "        # Sample clients which meet the criterion\n",
    "        available_cids = list(self.clients)\n",
    "        if selectedCriterion is not None:\n",
    "            available_cids = [\n",
    "                cid for cid in available_cids if selectedCriterion.select(self.clients[cid])\n",
    "            ]\n",
    "            \n",
    "        print(\"Number of selected clients to aggregation: \", len(available_cids))\n",
    "\n",
    "\n",
    "        if min_num_clients > len(available_cids):\n",
    "            print(\n",
    "                \"Sampling failed: number of available clients\",len(available_cids),\n",
    "                \" is less than number of requested clients.\", num_clients,\n",
    "            )\n",
    "            return []\n",
    "        \n",
    "        sampled_cids = random.sample(available_cids, min_num_clients)\n",
    "        return [self.clients[cid] for cid in sampled_cids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf426049",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Start simulation\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=NUMBER_OF_CLIENTS,\n",
    "    config=fl.server.ServerConfig(num_rounds=NUMBER_OF_ITERATIONS),  # Just three rounds\n",
    "    client_resources=client_resources,\n",
    "    strategy = strategy,\n",
    "    client_manager = CustomSimpleClientManager()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
